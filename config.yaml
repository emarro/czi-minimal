# Data configuration
data_local: #./data
data_remote: "kuleshov-group/Angiosperm_16_genomes"
eval_remote: "emarro/maize-allele-frequency-20k-val"
max_seq_len: 512
run_name: "pcad1_from_scratch_${pretrained_name_or_path}"
mlm_probability: 0.15 # prob of masking a given token in an input sequence
mask_replace_prob: 0.80 # prob of replacing a masked token with [MASK]
random_replace_prob: 0.10 #prob of replacing a masked token with another token in the vocab

repeat_weight: 0.10 #downweight losses for soft-masked DNA regions

from_pretrained: True
from_scratch: True
pretrained_name_or_path: "kuleshov-group/PlantCaduceus_l20"

# Model configuration
model:
  pad_token_id: -100  # [PAD] token ID
  model_config:
    d_model: 128
    n_layer: 1 #8
    # The pretrained caduceus models use the native `CaduceusTokenizer` which
    # has a vocab size of 12 tokens and must be padded to a multiple of
    # `pad_vocab_size_multiple`; see:
    # https://huggingface.co/kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16/blob/main/tokenization_caduceus.py
    vocab_size: 16
    # This is the multiple used in the pretrained models, e.g.:
    # https://huggingface.co/kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16/blob/main/config.json#L42
    pad_vocab_size_multiple: 8  # Ensure vocab size is multiple of 8
    # This is important because even though padding is not used in caduceus pretraining,
    # the loss functions are configured to use this sentinel index to ignore any special tokens at:
    # https://github.com/kuleshov-group/caduceus/blob/2593c807ebfc0aede05e7a991474b1a0e60fdc93/caduceus/modeling_caduceus.py#L477-L482
    # This can be confusing because all tokenizers used with Caduceus (at TOW) also have a pad token with an index that is not -100,
    # e.g. `CaduceusTokenizer` has a pad token with index 1 and the songlab/tokenizer-dna-mlm tokenizer uses 0.

# Training configuration
seed: 42
max_duration: "50ep" #"500ba"  # Train for 50 epochs
eval_interval: "5000ba"  # Evaluate every 5k batches
global_train_batch_size: 2048

# Use a fixed microbatch size to avoid this warning, which does become an error
# in the evironment configured for this project:
# /home/ubuntu/repos/composer/composer/trainer/trainer.py:247: UserWarning: `device_train_microbatch_size='auto'`
#   may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory
#   errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to
#   PyTorch bugs, e.g. integer overflow.  In this case, please manually set device_train_microbatch_size explicitly
#   to an integer instead.
device_train_microbatch_size: "auto"
# device_train_microbatch_size: 8

# Use float32 precision because bfloat16 (bf16) requires newer GPUs than the Tesla GPU
# configured for this environment.  Using this precision requires Ampere or later generations,
# and this error occurs otherwise: "Feature '.bf16' requires .target sm_80 or higher".  In short:
# - fp32: works on all GPUs but uses more memory
# - amp_bf16: requires Ampere (A100) or newer GPUs (sm_80+)
# - amp_fp16: alternative for older GPUs but less numerically stable
# precision: amp_bf16 # update for Composer release/v0.12.1
precision: "amp_bf16"

# Optimizer configuration
optimizer:
  lr: 2.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-6
  weight_decay: 1.0e-5

# Learning rate scheduler
scheduler:
  t_warmup: "1000ba"
  alpha_f: 0.02

# Data loading
train_loader:
  num_workers: 2

# Checkpointing
save_folder: "./checkpoints/${run_name}" # e.g. './{run_name}/ckpt' (local) or 's3://mybucket/mydir/{run_name}/ckpt' (remote)
save_interval: "5000ba"
save_num_checkpoints_to_keep: 1

#   (Optional) W&B logging
#loggers:
#  wandb:
#    project: "Composer Eval Test"
#    entity: "emarro"


#   (Optional) Load from local filesystem or remote object store to
#  start from an existing model checkpoint;
#   e.g. './ckpt/latest-rank{rank}.pt' (local), or
#   's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
# load_path: "./checkpoints/${run_name}/latest-rank{rank}.pt"
