# Data configuration
data_local: #./data
data_remote: "kuleshov-group/Angiosperm_16_genomes"
eval_remote: "emarro/maize-allele-frequency-20k-val"
max_seq_len: 512
run_name: "test_hnet"
mlm: False # whether we're in an MLM masking setup
default_target_ratio: 6 #N from HNet paper, returned per batch, default to 6 if now overwritten in code
hnet_model: True #Temporary flag to load hnet model

#unused if MLM is False
mlm_probability: 0.15 # prob of masking a given token in an input sequence
mask_replace_prob: 0.80 # prob of replacing a masked token with [MASK]
random_replace_prob: 0.10 #prob of replacing a masked token with another token in the vocab

repeat_weight: 0.10 #downweight losses for soft-masked DNA regions

from_pretrained: False
from_scratch: False
pretrained_name_or_path:  ""

# Model configuration
model:
  pad_token_id: -100  # [PAD] token ID
  ratio_loss_weight: 0.03 #alpha from the paper
  arch_layout: ["m3t1", ["M15"], "m4"] #architecture for DNA from Table 7 with best PPL in HNet paper

  #values below are all placeholder
  d_model: 128
  d_intermediate: [0, 1024]
  vocab_size: 16
  ssm_cfg:
    chunk_size: 256
    d_conv: 4
    d_state: 128
    expand: 2
  attn_cfg:
    num_heads: [16, 16]
    rotary_emb_dim: [32, 48]
    window_size: [1023, -1]
  tie_embeddings: False

# Training configuration
seed: 42
max_duration: "50ep" #"500ba"  # Train for 50 epochs
eval_interval: "5000ba"  # Evaluate every 5k batches
global_train_batch_size: 2048

# Use a fixed microbatch size to avoid this warning, which does become an error
# in the evironment configured for this project:
# /home/ubuntu/repos/composer/composer/trainer/trainer.py:247: UserWarning: `device_train_microbatch_size='auto'`
#   may potentially fail with unexpected CUDA errors. Auto microbatching attempts to catch CUDA Out of Memory
#   errors and adjust the batch size, but it is possible CUDA will be put into an irrecoverable state due to
#   PyTorch bugs, e.g. integer overflow.  In this case, please manually set device_train_microbatch_size explicitly
#   to an integer instead.
device_train_microbatch_size: "auto"
# device_train_microbatch_size: 8

# Use float32 precision because bfloat16 (bf16) requires newer GPUs than the Tesla GPU
# configured for this environment.  Using this precision requires Ampere or later generations,
# and this error occurs otherwise: "Feature '.bf16' requires .target sm_80 or higher".  In short:
# - fp32: works on all GPUs but uses more memory
# - amp_bf16: requires Ampere (A100) or newer GPUs (sm_80+)
# - amp_fp16: alternative for older GPUs but less numerically stable
# precision: amp_bf16 # update for Composer release/v0.12.1
precision: "amp_bf16"

# Optimizer configuration
optimizer:
  lr: 2.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-6
  weight_decay: 1.0e-5

# Learning rate scheduler
scheduler:
  t_warmup: "1000ba"
  alpha_f: 0.02

# Data loading
train_loader:
  num_workers: 2

# Checkpointing
save_folder: "./checkpoints/${run_name}" # e.g. './{run_name}/ckpt' (local) or 's3://mybucket/mydir/{run_name}/ckpt' (remote)
save_interval: "5000ba"
save_num_checkpoints_to_keep: 1

#   (Optional) W&B logging
#loggers:
#  wandb:
#    project: "Composer Eval Test"
#    entity: "emarro"


#   (Optional) Load from local filesystem or remote object store to
#  start from an existing model checkpoint;
#   e.g. './ckpt/latest-rank{rank}.pt' (local), or
#   's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
# load_path: "./checkpoints/${run_name}/latest-rank{rank}.pt"
