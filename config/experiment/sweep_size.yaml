# @package _global_
# let the experiment node modify configs globally
defaults:
  - override /model: hnet
  - override /trainer: fixed_batch
  - override /loggers: wandb

tags: ["hnet", "vary_d"]

hydra:
  mode: "MULTIRUN"
  sweeper:
    # hyperparams to search over
    params:
      model.default_target_ratio: 2, 3, 5, 10, 15, null
      model.d_model: 
        - [768, 768] #d_model of each stage (matches stages from arch_layout)
        - [1024, 1024]
        - [768, 512]





task_name: "sweep_size"

optimizer:
  lr: 5e-4

trainer:
  device_train_microbatch_size: 256 #depends on the amount of GPU mem available
  max_duration: "20ep"

loggers:
  wandb:
    tags: ${tags}
