# @package _global_
# let the experiment node modify configs globally
defaults:
  - override /model: hnet
  - override /trainer: fixed_batch
  - override /loggers: wandb



task_name: "debug_flops"

optimizer:
  lr: 5e-4

trainer:
  device_train_microbatch_size: 128  #depends on the amount of GPU mem available
  max_duration: "15ba"
  eval_interval: "20ba"  # Evaluate every 5k batches
  global_train_batch_size: 128
  save_overwrite: True
  autoresume: False
  reset_time: True

loggers:
  wandb:
    project: "Composer Debug"
